DEBUG : True
INIT_VAL : False
GREEDY_BASELINE : False
SAVE_BEST_METRIC : False # to save nothing : = False and checkpoint_freq = 0
ONLY_INFERENCE : False
ONLY_VAL : False

dataset : coco
captions_type : "mistral" #coco, (mistral, mistral_4), blip_holistic
train_method : "mle" # (discriminative, cider, mle)
prefix_len : 10
warmup_ratio : 0.034
official_clipcap_weights : /home/manugaur/official_clipcap/coco_weights.pt
num_workers : 16
CAPS_PER_IMG_train: 5  # tokenisation
CAPS_PER_IMG_val: 5  # NLG eval
diff_lr: False
fp16: False

mllm : "clipcap" # {llava, clipcap}
reinforce : False
contrastive : False
#hparams
lora : False
lora_rank : 32
log_spice : False

freeze_wte : False
freeze_adapter : False

#lora
lora : False
finetune_model : llm
lora_rank : 32
clip_mlp_ft : False
adapter_lora : False
adapter_rank : 32
clip_text : False

neg_mining:
  curricullum : {10 : ["rand", 5]} # "easy" : 3, "medium": 7, "hard" : 20} #indexed from 1 {"medium" : 3, "hard" : 10}
  save_optimizer : False
  bag_size : 5
  save_using_neg : False #save model i.e best on neg_dataloader 
  val_bag_size : 5
  val_level : "rand"

resume_training : 
  do : False
  dir : sr_easy_bsz5
  load_epoch  : 2 
  
opts: 
  baseline : mean
  n_epochs : 10
  batch_size : 2 #10 for ada
  lr :  2e-5
  max_len : 50
  checkpoint_freq : 0
  num_workers : 16
  dataset_dir : /home/manugaur/coco
  checkpoint_dir : /home/manugaur/EGG/checkpoints/mistral_4/mle_1cap
  recv_clip_model : ViT-B/32 # ViT-L/14@336px

WANDB :
  logging : False
  sweep : False
  sweep_id: ""
  sweep_run_count : 100
  entity : "manugaur"
  project : "emergent_captioner"
  run_name : "final" #suffix to "sr_{data}_"
 
inference : 
  batch_size : 100
  output_dir : /home/manugaur/EGG/inference_preds/

#DEBUG,INIT_VAL, logging, batch_size