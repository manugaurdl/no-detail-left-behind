DEBUG : True
INIT_VAL : False
GREEDY_BASELINE : False
SAVE_BEST_METRIC : True # to save nothing : = False and checkpoint_freq = 0
ONLY_INFERENCE : False
ONLY_VAL : False

CIDER_SR : False
dataset : coco
captions_type : "coco" #coco, (mistral, mistral_4), blip_holistic
train_method : "discriminative" # (discriminative, cider, mle)
prefix_len : 10
warmup_ratio : 0.034
official_clipcap_weights : /home/manugaur/official_clipcap/coco_weights.pt
num_workers : 16
CAPS_PER_IMG_train: 5  # tokenisation
CAPS_PER_IMG_val: 5  # NLG eval
diff_lr: False
fp16: False

mllm : "clipcap" # {llava, clipcap}
reinforce : True
contrastive : False

#lora
lora : True
finetune_model : both # {gpt, clip} # automatically freeze gpt.transformer or CLIP
lora_rank : 32
clip_mlp_ft : True
adapter_lora : False
adapter_rank : 32
clip_text : False

# freezing layers
freeze_wte : True
freeze_adapter : False
log_spice : False

curri: {1: 0} # {n_epoch : bag_size}
val_bag_size : 5

resume_training : 
  do : False
  dir : sr_easy_bsz5
  load_epoch  : 2 
  
opts: 
  baseline : mean
  batch_size : 8
  lr :  9e-8
  max_len : 50
  checkpoint_freq : 0
  num_workers : 16
  dataset_dir : /storage/datasets/coco
  mle_model_path : /workspace/manugaur/no-detail-left-behind/checkpoints/coco/mle_coco_repro/best.pt #it loads only sender.clipcap (adapter + llm)
  checkpoint_dir : /workspace/manugaur/no-detail-left-behind 
  recv_clip_model : ViT-B/32 # ViT-L/14@336px

WANDB :
  logging : False
  sweep : False
  sweep_id: ""
  sweep_run_count : 100
  entity : "manugaur"
  project : "emergent_captioner"
  run_name : "coco_sr_repro"
 
inference : 
  batch_size : 8
  output_dir : /workspace/manugaur/no-detail-left-behind

#DEBUG,INIT_VAL, logging, batch_size