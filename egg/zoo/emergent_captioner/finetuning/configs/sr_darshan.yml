DEBUG : False  # if true : model saving, logging turned off.  
INIT_VAL : True
GREEDY_BASELINE : False
SAVE_BEST_METRIC : False # to save nothing : = False and checkpoint_freq = 0
ONLY_INFERENCE : False
ONLY_VAL : False

captions_type : "coco" #coco, (mistral, mistral_4), blip_holistic
train_method : "discriminative" # (discriminative, cider, mle)
prefix_len : 10
warmup_ratio : 0.034
official_clipcap_weights : /home/manugaur/official_clipcap/coco_weights.pt
num_workers : 16
CAPS_PER_IMG_train: 1 # maybe use in CL
CAPS_PER_IMG_val: 4 # NLG eval
fp16: False
diff_lr: False
contrastive : True
reinforce : False

#lora
lora : True
lora_rank : 64 #64 for clip
clip_mlp_ft : True
clip_text : True

finetune_model : clip # {gpt, clip} # automatically freeze gpt.transformer or CLIP
freeze_wte : True
freeze_adapter : True

neg_mining:
  curricullum : {"rand": 2, "medium" : 7} #{"rand" : 10} indexed from 1 {"medium" : 3, "hard" : 10}
  bag_size : 5 
  save_using_neg : False #save model i.e best on neg_dataloader 
  val_bag_size : 5
  val_level : "medium"

opts: 
  baseline : mean
  n_epochs : 10
  batch_size : 250
  lr : 1e-7 #2e-5
  max_len : 50
  checkpoint_freq : 0
  num_workers : 16
  dataset_dir : /home/manugaur/coco
  mle_model_path : /home/manugaur/EGG/checkpoints/blip2mistral/mle/best.pt #it loads only sender.clipcap (adapter + llm)
  checkpoint_dir : /home/manugaur/EGG/checkpoints/blip2mistral/sr
    
WANDB:
  logging : True
  sweep : False
  sweep_id: ""
  sweep_run_count : 100
  entity : "darshan-singh"
  project : "emergent_captioner"
  run_name : CL_2R7M_bagsz5_plot_clip_zs_1e7 #suffix to "sr_{data}_"
  log_mmvp_all : True

inference : 
  batch_size : 128
  output_dir : /home/manugaur/EGG/inference_preds/

#DEBUG, ONLY_INFERENCE,  INIT_VAL, logging, save_using_neg
#CKPT_DIR = f"{data}/{sr}_{config[WANDB][run_name]}
